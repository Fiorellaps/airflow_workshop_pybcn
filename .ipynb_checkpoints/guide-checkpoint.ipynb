{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec09b36d-fc61-4184-8ad9-9f0cb88da334",
   "metadata": {},
   "source": [
    "# 1. Instalación de Airflow en Linux"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6f8996-13e4-43c9-91e1-f67d7194886e",
   "metadata": {},
   "source": [
    "- Creamos un entorno de Python con virtualenv y lo activamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "006fca2d-46fb-4702-8aaf-ed7087b07a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m venv airflow_env\n",
    ">source airflow_env/bin/ativate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c419f423-6cda-44b0-9e39-9fafb969ee9a",
   "metadata": {},
   "source": [
    "- Definimos **AIRFLOW_HOME**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3be0424-27d0-43a5-a64a-1b2eafd5f687",
   "metadata": {},
   "outputs": [],
   "source": [
    ">export AIRFLOW_HOME=~/airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fb7025-bd37-4b20-bf30-68cf635711d3",
   "metadata": {},
   "source": [
    "- Instalamos airflow desde Pypi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8588c0d2-0380-41f5-9831-514b53362bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"apache-airflow[gcp]\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0416ea34-b79b-4633-b953-463196a0e1b4",
   "metadata": {},
   "source": [
    "<div style=\"color:green\">Airflow ofrece muchos más <a href=\"https://airflow.apache.org/docs/#providers-packagesdocsapache-airflow-providersindexhtml\">providers</a>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763c3f97-e32d-4505-976e-50e8c8fdb54b",
   "metadata": {},
   "source": [
    "- Modificar la configuración de airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "187cdc54-5977-49d1-be3d-4eb4ce33309e",
   "metadata": {},
   "outputs": [],
   "source": [
    ">nano /root/airflow/airflow.cfg\n",
    "#- load_examples=False\n",
    "#- sql_alchemy_conn = postgresql+psycopg2://user:pass@localhost:5432/airflow_db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc96c08-7a74-4daf-b113-054f5d0fcfc2",
   "metadata": {},
   "source": [
    "- Comprobamos que está bien instalado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc64f9-b72c-4e8c-8168-5f4b8ebc7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "airflow version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9c2d7f-1bb9-42b0-a9b6-25c74000e063",
   "metadata": {},
   "source": [
    "- Arrancamos airflow manualmente\n",
    "<div style=\"color:green\">** También se podría arrancar usando el comando `airflow standalone` o en modo cluster\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53704954-35b0-495f-8306-52fb3e53d0e6",
   "metadata": {},
   "source": [
    "# 2. Arrancar Airflow\n",
    "\n",
    "## 1.1. Base de datos (Postgres, SQLite, etc.)\n",
    "\n",
    "\n",
    "<div style=\"color:green\">\n",
    "** Más información en https://airflow.apache.org/docs/apache-airflow/stable/howto/set-up-database.html\n",
    "</div>\n",
    "\n",
    "##### PREPARAR EL POSTGRES\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "    \n",
    "    \n",
    "- CREATE DATABASE airflow_db;\n",
    "\n",
    "- CREATE USER airflow_user WITH PASSWORD 'XXX';\n",
    "    \n",
    "- GRANT ALL PRIVILEGES ON DATABASE airflow_db TO airflow_user;\n",
    "    \n",
    "- ALTER ROLE airflow_user SET search_path = public;    \n",
    "   \n",
    "    \n",
    "</div>\n",
    "\n",
    "##### INICIALIZAR POSTGRES\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "> airflow db init \n",
    "</div>\n",
    "\n",
    "\n",
    "##### CREAR USUARIO\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "> airflow users create --username admin --firstname admin --lastname admin --role Admin --email admin@admin.org \n",
    "</div>\n",
    "\n",
    "##### COMPROBAR USUARIO\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "> airflow users list \n",
    "</div>\n",
    "\n",
    "## 1.2. Webserver\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "> airflow webserver --port 8080\n",
    "</div>\n",
    "\n",
    "\n",
    "## 1.3. Scheduler\n",
    "\n",
    "<div style=\"color:orange\">\n",
    "> airflow scheduler\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f5266c-867c-4f02-9b32-b5ac029d38eb",
   "metadata": {},
   "source": [
    "# 3. DAGS\n",
    "\n",
    "## 01-check_file_dag.py\n",
    "\n",
    "Comprobar que un fichero existe en la ruta dada. Para ello usamos un **BashOperator** que ejecuta un script de bash.\n",
    "\n",
    "Con este DAG aprenderemos:\n",
    "\n",
    "- Configurar Dags: parámetro, intervalos, programación...\n",
    "\n",
    "- Visualización de Dags en la web: Ver programación, estado del DAG, historial, logs...\n",
    "\n",
    "- Añadir markdown en DAG y las tareas (instance details).\n",
    "\n",
    "- BashOperator.\n",
    "\n",
    "## 02-load-csv-to-GC.py\n",
    "\n",
    "Subir un fichero (en este caso csv) a un bucket de **Google Cloud Storage**.\n",
    "\n",
    "Con este DAG aprenderemos:\n",
    "\n",
    "- PythonOperator.\n",
    "\n",
    "- Configuración de variables.\n",
    "\n",
    "- Organización de tareas de un DAG.\n",
    "\n",
    "## 02_load_csv_to_GC.py\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
